{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modularized import Model\n",
    "import os\n",
    "import jsonlines\n",
    "\n",
    "# Set Together API key \n",
    "os.environ[\"TOGETHER_API_KEY\"] = \"random\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort PIDs based on length of notes & number of notes \n",
    "\n",
    "level1_note1_pids = []\n",
    "level1_note2_pids = []\n",
    "level2_note1_pids = []\n",
    "level2_note2_pids = []\n",
    "level2_note3_pids = []\n",
    "\n",
    "with jsonlines.open('EHRNoteQA.jsonl') as reader:\n",
    "    # Cycle through QAs and sort pids based on level and pid \n",
    "    for obj in reader:\n",
    "        pid = obj[\"patient_id\"]\n",
    "        if obj['category'] == 'level1': \n",
    "            if obj['num_notes'] == 1: \n",
    "                level1_note1_pids.append(pid)\n",
    "            elif obj['num_notes'] == 2: \n",
    "                level1_note2_pids.append(pid)\n",
    "        elif obj['category'] == 'level2':\n",
    "            if obj['num_notes'] == 1: \n",
    "                level2_note1_pids.append(pid)\n",
    "            elif obj['num_notes'] == 2: \n",
    "                level2_note2_pids.append(pid)\n",
    "            elif obj['num_notes'] == 3: \n",
    "                level2_note3_pids.append(pid)\n",
    "\n",
    "#print(level1_note1_pids) \n",
    "#print(level1_note2_pids) \n",
    "#print(level2_note1_pids) \n",
    "#print(level2_note2_pids) \n",
    "#print(level2_note3_pids) \n",
    "\n",
    "print(len(level1_note1_pids))\n",
    "print(len(level1_note2_pids))\n",
    "print(len(level2_note1_pids)) \n",
    "print(len(level2_note2_pids)) \n",
    "print(len(level2_note3_pids)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset (train, validation, test split)\n",
    "\n",
    "# Each set has 4 patients with 1 note, 4 patients with 2 notes, and 4 patients with 3 notes \n",
    "train_set = level1_note1_pids[:2] + level2_note1_pids[:2] + level1_note2_pids[:2] + level2_note2_pids[:2] + level2_note3_pids[:4]\n",
    "validation_set = level1_note1_pids[2:4] + level2_note1_pids[2:4] + level1_note2_pids[2:4] + level2_note2_pids[2:4] + level2_note3_pids[4:8]\n",
    "test_set = level1_note1_pids[4:6] + level2_note1_pids[4:6] + level1_note2_pids[4:6] + level2_note2_pids[4:6] + level2_note3_pids[8:12]\n",
    "dataset = train_set + validation_set + test_set \n",
    "\n",
    "print(train_set)\n",
    "print(len(train_set))\n",
    "print(validation_set)\n",
    "print(len(validation_set))\n",
    "print(test_set)\n",
    "print(len(test_set))\n",
    "print(dataset)\n",
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model system components \n",
    "# Data:       Loads initial patient database from file + preprocesses by chunking notes \n",
    "# Embedder:   Loads embedding model + can embed data \n",
    "# Storage:    Populates vector databases w/ patient data + can query indices \n",
    "# Generator:  Loads LLM model + can generate responses (untested!)\n",
    "\n",
    "data_path = \"C:/Users/sharp/Documents/Research/Adelaide/clinical_RAG/dataset.csv\"\n",
    "embed_model = \"ClinicalBERT\"\n",
    "llm_model = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
    "api_key = os.environ[\"TOGETHER_API_KEY\"]\n",
    "\n",
    "model = Model(data_path, embed_model, api_key, llm_model)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataset stats  \n",
    "model.data.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates precision, recall, and F1 given you have counts for each sample of retreived + relevant (TP), retrieved (TP+FP), relevant (TP+FP)\n",
    "def print_metrics(correct_counts, total_retrieved_counts, total_correct_counts, total_counts): \n",
    "    \n",
    "    print(correct_counts)\n",
    "    print(total_retrieved_counts)\n",
    "    print(total_correct_counts)\n",
    "    print(total_counts)\n",
    "\n",
    "    num_samples = len(correct_counts)\n",
    "   \n",
    "    metrics = []\n",
    "\n",
    "    # TODO: handle edges cases \n",
    "    for i in range(num_samples): \n",
    "\n",
    "        # Get counts for sample \n",
    "        correct = correct_counts[i]\n",
    "        total_retrieved = total_retrieved_counts[i]\n",
    "        total_correct = total_correct_counts[i]\n",
    "\n",
    "        # If no chunks retrieved -> 100 precision (0/0)\n",
    "        # If chunks retrived -> calculate precision (?/#)\n",
    "        if total_retrieved == 0: prec = 100 \n",
    "        else: prec = (correct/total_retrieved) * 100 \n",
    "\n",
    "        # If no chunks relevant, and none retrieved -> 100 recall (0/0)\n",
    "         # If no chunks relevant, yet some retrieved -> error      (#/0)\n",
    "        # If chunks relevant, retrieved unknown -> calculate recall      (?/#)\n",
    "        if total_correct == 0: \n",
    "            if correct == 0: rec = 100 \n",
    "            else: print(\"error\")\n",
    "        else: rec = (correct/total_correct) * 100\n",
    "\n",
    "        f1 = 2*(prec * rec)/(prec + rec)\n",
    "\n",
    "        # Update total precision, recall, f1 \n",
    "        metrics.append((prec, rec, f1))\n",
    "\n",
    "    avg_p = sum(m[0] for m in metrics) / num_samples\n",
    "    avg_r = sum(m[1] for m in metrics) / num_samples\n",
    "    avg_f1 = sum(m[2] for m in metrics) / num_samples\n",
    "\n",
    "    print(f\"\\nAverage precision: {avg_p}\")\n",
    "    print(f\"Average recall: {avg_r}\")\n",
    "    print(f\"F1: {avg_f1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_set(pids, threshold_eval, threshold_search):\n",
    "\n",
    "    TP_all = []\n",
    "    TP_correct = []\n",
    "\n",
    "    retrieved = []\n",
    "    total_chunks = []\n",
    "\n",
    "    relevant_all = []\n",
    "    relevant_correct = []\n",
    "\n",
    "    \n",
    "    with jsonlines.open('EHRNoteQA.jsonl') as reader:\n",
    "        # Cycle throuh QAs corresponding with subset \n",
    "        for obj in reader:\n",
    "            pid = obj[\"patient_id\"] \n",
    "            if pid in pids:\n",
    "                query = obj[\"question\"]\n",
    "                answer =  obj[\"answer\"]\n",
    "                answer_choices = {'A': obj[\"choice_A\"], 'B': obj[\"choice_B\"], 'C': obj[\"choice_C\"], 'D': obj[\"choice_D\"], 'E': obj[\"choice_E\"]}\n",
    "\n",
    "                #print(f\"Question:\\n{query}\\n\")\n",
    "                #print(f\"Answer choices:\\n{answer_choices}\\n\")\n",
    "                #print(f\"Correct answer: {answer}\\n\")\n",
    "\n",
    "                # Step 1: \n",
    "                # Retrieve patient data using query\n",
    "                retrieved_chunks = model.storage.search_index(pid, query, threshold_search)\n",
    "\n",
    "                # Evaluate \n",
    "                all, correct, total_retrieved = model.storage.evaluate_retrieved(pid, retrieved_chunks, answer_choices, answer, threshold_eval) # By default also prints \n",
    "                total_all, total_correct, total = model.storage.evaluate_relevant(pid, answer_choices, answer, threshold_eval)\n",
    "\n",
    "                TP_all.append(all)\n",
    "                TP_correct.append(correct) \n",
    "                retrieved.append(total_retrieved)\n",
    "                relevant_all.append(total_all)\n",
    "                relevant_correct.append(total_correct)\n",
    "                total_chunks.append(total)\n",
    "\n",
    "            \n",
    "                # Step 2: \n",
    "                # Generate output using retreived patient data and query (confirm if this works first in normal pipeline)\n",
    "                # answer = model.generator.generate_response_from_chunks(query, retrieved)\n",
    "                # print(\"== RESPONSE:==\")\n",
    "                # print(answer)\n",
    "\n",
    "                # Evaluate generated output \n",
    "\n",
    "    print('Retrieval Metrics if all answers are considered relevant:\\n')\n",
    "    print_metrics(TP_all, retrieved, relevant_all, total_chunks) \n",
    "\n",
    "    print('Retrieval Metrics if only correct answer is considered relevant:\\n')\n",
    "    print_metrics(TP_correct, retrieved, relevant_correct, total_chunks) \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: need to tune after doing some data exploration \n",
    "#evaluate_set(train_set, 0.85, 55)\n",
    "#evaluate_set(validation_set, .85, 55)\n",
    "evaluate_set(train_set, 0.80, 60)\n",
    "evaluate_set(validation_set, .80, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_set(test_set, .80, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
