{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O4PMxoB_pXZZ"
      },
      "outputs": [],
      "source": [
        "## 1. Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXBQHvhko8_q",
        "outputId": "261c4a44-44b8-434a-9ab7-4d1a804adf75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import faiss\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from together import Together\n",
        "from typing import List, Dict, Tuple\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsD2D8JDo9l9",
        "outputId": "0e81fdad-d4ee-4110-8445-43e1960926c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "os.environ[\"TOGETHER_API_KEY\"] = st.secrets[\"TOGETHER_API_KEY\"]\n",
        "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = st.secrets[\"HUGGINGFACE_HUB_TOKEN\"]\n",
        "from huggingface_hub import login\n",
        "login(token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y7-aM6FpUqp"
      },
      "source": [
        "## 2. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6f6bluFpB_L",
        "outputId": "9714f99c-a71b-471e-a451-d628f61b2f44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "# notebook and the CSV are in 'clinical_RAG' folder\n",
        "file_path = '/content/task_data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woymu7-JpIq7",
        "outputId": "fddcbf7c-9a1b-488a-9dc1-74e13f2939ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "# config\n",
        "BIOBERT_MODEL = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "EMBED_DIM = 768  # BioBERT output dim\n",
        "# load models\n",
        "tokenizer_biobert = AutoTokenizer.from_pretrained(BIOBERT_MODEL)\n",
        "biobert = AutoModel.from_pretrained(BIOBERT_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljKV5aJcpNGT"
      },
      "source": [
        "# 3. ClinicalBERT Embedding Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCiaVFg3pMjG",
        "outputId": "218adb7b-b0de-42c2-ad45-64ec138941ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "def get_embedding(text: str) -> np.ndarray:\n",
        "    inputs = tokenizer_biobert(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = biobert(**inputs)\n",
        "    attention_mask = inputs['attention_mask']\n",
        "    last_hidden = outputs.last_hidden_state\n",
        "    mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
        "    masked_output = last_hidden * mask\n",
        "    mean_pooled = masked_output.sum(1) / mask.sum(1)\n",
        "    return mean_pooled.squeeze().cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyFm1W-hpdIy"
      },
      "source": [
        "# 4. FAISS Index Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrvn2QsRpe_d",
        "outputId": "7b4f86e0-22a0-4585-ecca-84ce56a383e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "\n",
        "def build_faiss_index(embeddings: List[np.ndarray]) -> faiss.IndexFlatL2:  # Change to faiss.IndexFlat\n",
        "    \"\"\"\n",
        "    Builds a FAISS index using the provided embeddings.\n",
        "\n",
        "    Args:\n",
        "        embeddings: A list of NumPy arrays representing the embeddings.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS index object (faiss.IndexFlat).\n",
        "    \"\"\"\n",
        "    dimension = embeddings[0].shape[0]  # Get dimensionality from embeddings\n",
        "    index = faiss.IndexFlat(dimension)  # Change to faiss.IndexFlat\n",
        "    index.add(np.vstack(embeddings))  # Add embeddings to the index\n",
        "    return index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGEQcbk_pg0e"
      },
      "source": [
        "## 5. Text Chunking\n",
        "Using Note Sections/Headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuULSKH1piyD",
        "outputId": "9b7fba9a-9710-4cb1-baef-1bd24c8036e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "def chunk_text(input_text: str, charttime, max_chars: int = 500) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Chunks a clinical text into smaller pieces based on sections and a maximum\n",
        "    character limit.\n",
        "\n",
        "    Args:\n",
        "        input_text: The clinical text to be chunked.\n",
        "        max_chars: The maximum number of characters allowed in each sub-chunk.\n",
        "\n",
        "    Returns:\n",
        "        A list of (section, chunk text, date) tuples.\n",
        "    \"\"\"\n",
        "\n",
        "    section_headers = [\n",
        "        \"Service:\", \"Allergies:\",\n",
        "        \"Chief Complaint:\", \"Major Surgical or Invasive Procedure:\", \"History of Present Illness:\",\n",
        "        \"Past Medical History:\", \"Social History:\", \"Family History:\", \"Physical Exam:\",\n",
        "        \"PHYSICAL EXAM ON ADMISSION:\", \"PHYSICAL EXAM ON DISCHARGE:\", \"Pertinent Results:\",\n",
        "        \"Brief Hospital Course:\", \"Medications on Admission:\", \"Discharge Medications:\",\n",
        "        \"Discharge Disposition:\", \"Facility:\", \"Discharge Diagnosis:\", \"Discharge Condition:\",\n",
        "        \"Discharge Instructions:\", \"Followup Instructions:\"\n",
        "    ]\n",
        "\n",
        "    pattern = re.compile(rf\"^({'|'.join(map(re.escape, section_headers))})\", re.MULTILINE)\n",
        "    matches = list(pattern.finditer(input_text))\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(len(matches)):\n",
        "        start = matches[i].start()\n",
        "        end = matches[i + 1].start() if i + 1 < len(matches) else len(input_text)\n",
        "        header = matches[i].group(1)\n",
        "        content = input_text[start:end].strip()\n",
        "\n",
        "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', content)\n",
        "        current_chunk = \"\"\n",
        "        for sentence in sentences:\n",
        "            if len(current_chunk) + len(sentence) <= max_chars:\n",
        "                current_chunk += sentence\n",
        "            else:\n",
        "                chunks.append((header, current_chunk.strip(), charttime))\n",
        "                current_chunk = sentence\n",
        "    chunks.append((header, current_chunk.strip(), charttime))\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2IGK_sPpsd0"
      },
      "source": [
        "chunk structure:\n",
        "```\n",
        "all_chunks = {\n",
        "  # patient  : list of their note chunks + metadata\n",
        "  subject_id : [(section, chunk text, time)]\n",
        "  \n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1mHdUPtpv2Z"
      },
      "source": [
        "## 7. Querying with FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YesE_rxDpzdd",
        "outputId": "0ff44481-4cf5-455f-b1b7-14d0aa443e41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "# Constants\n",
        "SCORE_THRESHOLD = 50   # Adjust based on empirical observation\n",
        "MAX_CHUNKS = 15\n",
        "\n",
        "def search_chunks_with_faiss(all_chunks, subject_id, query, top_k=30):\n",
        "    \"\"\"\n",
        "    Searches FAISS and filters chunks by score and count.\n",
        "    Assumes all_chunks[subject_id] = list of tuples (text, date).\n",
        "    \"\"\"\n",
        "    all_texts_with_dates = all_chunks.get(subject_id, [])\n",
        "    if not all_texts_with_dates:\n",
        "        print(f\"No chunks found for subject ID: {subject_id}\")\n",
        "        return []\n",
        "\n",
        "    # Build index using only the chunk text part of the tuples\n",
        "    all_texts = [text for section, text, date in all_texts_with_dates]\n",
        "\n",
        "    # embed all chunks\n",
        "    embeddings = [get_embedding(text) for text in all_texts]\n",
        "    index = build_faiss_index(embeddings) # build FAISS index from embeddings\n",
        "\n",
        "    # Search\n",
        "    query_emb = get_embedding(query) #embed query and use to search\n",
        "    D, I = index.search(query_emb.reshape(1, -1), top_k)\n",
        "\n",
        "    # Filter by score and cap to max chunks, include date\n",
        "    results = []\n",
        "    for i, score in zip(I[0], D[0]):\n",
        "        if score <= SCORE_THRESHOLD:\n",
        "            # Get section, text, and date from the original list using i\n",
        "            section, text, date = all_texts_with_dates[i]\n",
        "            results.append((text, section, date, score))\n",
        "            if len(results) >= MAX_CHUNKS:\n",
        "                break\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guRhgP5op4dG"
      },
      "source": [
        "## 8. Response Generation with LLaMA\n",
        " + display in streamlit app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxTJEW9up4Lh",
        "outputId": "6600bbd0-7388-4feb-f9c9-0805ff3a18ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "os.environ[\"TOGETHER_API_KEY\"] = os.environ[\"TOGETHER_API_KEY\"]\n",
        "client = Together(api_key=os.environ[\"TOGETHER_API_KEY\"])\n",
        "MODEL_NAME = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "By3Dxh8sqNvX",
        "outputId": "25a7a4d4-753b-419a-ef1f-1edc3ee35743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "def generate_response_from_chunks(query: str, retrieved_chunks: List[tuple]) -> str:\n",
        "    \"\"\"\n",
        "    Generates a clinically relevant, faithful, and date-aware summary using retrieved context.\n",
        "    \"\"\"\n",
        "    # Format each chunk as a date-tagged clinical note\n",
        "    context = \"\\n\".join([f\"[{section} {date}] {text.strip()}\" for text, section, date, score in retrieved_chunks])\n",
        "\n",
        "    # Optimized prompt\n",
        "    prompt = f\"\"\"You are a clinical assistant helping summarize a patient's medical history for a physician during clinical assessment.\n",
        "\n",
        "Patient Timeline (each entry includes a section, date, and note):\n",
        "{context}\n",
        "\n",
        "Query: \"{query}\"\n",
        "\n",
        "Instructions:\n",
        "- Extract only relevant and factual information from the timeline.\n",
        "- Summarize findings related to the query in clinical terms.\n",
        "- Associate each finding with its date.\n",
        "- Do not hallucinate or infer conditions that are not explicitly mentioned.\n",
        "- Present the response in a clear, concise manner suitable for use in clinical decision-making.\n",
        "\n",
        "Answer:\"\"\"\n",
        "    # Print the length of the prompt\n",
        "    print(\"== PROMPT:==\")\n",
        "    print(prompt)\n",
        "\n",
        "    # Print the length of the prompt\n",
        "    print(f\"Prompt length: {len(prompt)}\")\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsGSce7oqBhs",
        "outputId": "437c893c-bc73-41f8-e5b7-a1df44763a84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "\n",
        "# Streamlit app structure\n",
        "st.set_page_config(page_title=\"Clinical Chatbot\", layout=\"centered\")\n",
        "st.title(\"ðŸ©º Clinical Chatbot Assistant\")\n",
        "\n",
        "# Subject ID input (fixed or from dropdown in future)\n",
        "subject_id_to_search = st.number_input(\"Patient Subject ID\", value=2)\n",
        "\n",
        "# Initialize session state\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Initialize chunk dictionary only once\n",
        "if \"all_chunks\" not in st.session_state:\n",
        "    all_chunks = {}\n",
        "    for index, row in df.iterrows():\n",
        "        subject_id = row['patient_id']\n",
        "        if subject_id == subject_id_to_search:\n",
        "            note = row['text']\n",
        "            charttime = row['charttime']\n",
        "            chunks = chunk_text(note, charttime)\n",
        "            all_chunks[subject_id] = chunks\n",
        "    st.session_state.all_chunks = all_chunks\n",
        "\n",
        "# Display chat history\n",
        "for msg in st.session_state.messages:\n",
        "    with st.chat_message(msg[\"role\"]):\n",
        "        st.markdown(msg[\"content\"])\n",
        "\n",
        "# Chat input box\n",
        "user_query = st.chat_input(\"Enter your clinical question...\", value=\"How was physical therapy utilized during the hospital course?\")\n",
        "\n",
        "if user_query:\n",
        "    # Display user message\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_query})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(user_query)\n",
        "\n",
        "    # Process user query\n",
        "    with st.spinner(\"Searching notes and generating response...\"):\n",
        "        retrieved_chunks = search_chunks_with_faiss(\n",
        "            st.session_state.all_chunks,\n",
        "            subject_id_to_search,\n",
        "            user_query\n",
        "        )\n",
        "        response = generate_response_from_chunks(user_query, retrieved_chunks)\n",
        "\n",
        "    # Display assistant response\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo7llzaNsCaQ",
        "outputId": "3b54d680-b06a-43bd-db9e-80b547a23600"
      },
      "outputs": [],
      "source": [
        "# !npm install localtunnel\n",
        "# !streamlit run app.py &>/content/logs.txt &\n",
        "# !npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JDv9IcLxvvr"
      },
      "source": [
        "Testing Inputs:\n",
        "- Password: 34.46.47.233\n",
        "\n",
        "- Query: Does the patient show signs of neurological issues like face numbness?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "adelaide_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
