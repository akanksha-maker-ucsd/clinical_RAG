{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y5K3RrK806iT",
        "outputId": "8b4fd211-8f2b-4d9c-9c7b-5306a1ba4d23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pipreqs\n",
            "  Downloading pipreqs-0.5.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting docopt==0.6.2 (from pipreqs)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ipython==8.12.3 (from pipreqs)\n",
            "  Downloading ipython-8.12.3-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: nbconvert<8.0.0,>=7.11.0 in /usr/local/lib/python3.11/dist-packages (from pipreqs) (7.16.6)\n",
            "Collecting yarg==0.1.9 (from pipreqs)\n",
            "  Downloading yarg-0.1.9-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython==8.12.3->pipreqs) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython==8.12.3->pipreqs) (4.4.2)\n",
            "Collecting jedi>=0.16 (from ipython==8.12.3->pipreqs)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython==8.12.3->pipreqs) (0.1.7)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython==8.12.3->pipreqs) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.11/dist-packages (from ipython==8.12.3->pipreqs) (3.0.50)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from ipython==8.12.3->pipreqs) (2.18.0)\n",
            "Collecting stack-data (from ipython==8.12.3->pipreqs)\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.11/dist-packages (from ipython==8.12.3->pipreqs) (5.7.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython==8.12.3->pipreqs) (4.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from yarg==0.1.9->pipreqs) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.7.1)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (3.1.6)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (5.10.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (24.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (1.5.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (1.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython==8.12.3->pipreqs) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (4.3.7)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (6.1.12)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (4.23.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython==8.12.3->pipreqs) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython==8.12.3->pipreqs) (0.2.13)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert<8.0.0,>=7.11.0->pipreqs) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->yarg==0.1.9->pipreqs) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->yarg==0.1.9->pipreqs) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->yarg==0.1.9->pipreqs) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->yarg==0.1.9->pipreqs) (2025.1.31)\n",
            "Collecting executing>=1.2.0 (from stack-data->ipython==8.12.3->pipreqs)\n",
            "  Downloading executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack-data->ipython==8.12.3->pipreqs)\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pure-eval (from stack-data->ipython==8.12.3->pipreqs)\n",
            "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.24.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (24.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.8.2)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (6.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (1.17.0)\n",
            "Downloading pipreqs-0.5.0-py3-none-any.whl (33 kB)\n",
            "Downloading ipython-8.12.3-py3-none-any.whl (798 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.3/798.3 kB\u001b[0m \u001b[31m317.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m449.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=497e86d97bb245685b122b630b8c97a5ae7231cf54f62d4d22e02c214da54339\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built docopt\n",
            "Installing collected packages: pure-eval, docopt, jedi, executing, asttokens, yarg, stack-data, ipython, pipreqs\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 7.34.0\n",
            "    Uninstalling ipython-7.34.0:\n",
            "      Successfully uninstalled ipython-7.34.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 8.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asttokens-3.0.0 docopt-0.6.2 executing-2.2.0 ipython-8.12.3 jedi-0.19.2 pipreqs-0.5.0 pure-eval-0.2.3 stack-data-0.6.3 yarg-0.1.9\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "cb9f47542846492e9fb618daa494cf8d",
              "pip_warning": {
                "packages": [
                  "IPython"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Not scanning for jupyter notebooks.\n",
            "WARNING: Import named \"beautifulsoup4\" not found locally. Trying to resolve it at the PyPI server.\n",
            "WARNING: Import named \"beautifulsoup4\" was resolved to \"beautifulsoup4:4.13.3\" package (https://pypi.org/project/beautifulsoup4/).\n",
            "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
            "WARNING: Import named \"numpy\" not found locally. Trying to resolve it at the PyPI server.\n",
            "WARNING: Import named \"numpy\" was resolved to \"numpy:2.2.4\" package (https://pypi.org/project/numpy/).\n",
            "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
            "WARNING: Import named \"pandas\" not found locally. Trying to resolve it at the PyPI server.\n",
            "WARNING: Import named \"pandas\" was resolved to \"pandas:2.2.3\" package (https://pypi.org/project/pandas/).\n",
            "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
            "WARNING: Import named \"python_magic\" not found locally. Trying to resolve it at the PyPI server.\n",
            "WARNING: Import named \"python_magic\" was resolved to \"python-magic:0.4.27\" package (https://pypi.org/project/python-magic/).\n",
            "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
            "WARNING: Import named \"Requests\" not found locally. Trying to resolve it at the PyPI server.\n",
            "WARNING: Import named \"Requests\" was resolved to \"requests:2.32.3\" package (https://pypi.org/project/requests/).\n",
            "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
            "WARNING: Import named \"together\" not found locally. Trying to resolve it at the PyPI server.\n",
            "WARNING: Import named \"together\" was resolved to \"together:1.5.5\" package (https://pypi.org/project/together/).\n",
            "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
            "INFO: Successfully saved requirements file in ./requirements.txt\n"
          ]
        }
      ],
      "source": [
        "!pip install pipreqs\n",
        "!pipreqs . --force"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4PMxoB_pXZZ"
      },
      "outputs": [],
      "source": [
        "## 1. Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7afthHqnEf5",
        "outputId": "0306efe7-bd60-441f-cf05-4d8351266917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.46.47.233\n"
          ]
        }
      ],
      "source": [
        "!curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lsu1oqWup53"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/clinical_RAG/discharge_notes_40_patients.csv /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXBQHvhko8_q",
        "outputId": "261c4a44-44b8-434a-9ab7-4d1a804adf75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import faiss\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from together import Together\n",
        "from typing import List, Dict, Tuple\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsD2D8JDo9l9",
        "outputId": "0e81fdad-d4ee-4110-8445-43e1960926c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "os.environ[\"TOGETHER_API_KEY\"] = st.secrets[\"TOGETHER_API_KEY\"]\n",
        "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = st.secrets[\"HUGGINGFACE_HUB_TOKEN\"]\n",
        "from huggingface_hub import login\n",
        "login(token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y7-aM6FpUqp"
      },
      "source": [
        "## 2. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6f6bluFpB_L",
        "outputId": "9714f99c-a71b-471e-a451-d628f61b2f44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "# Auth\n",
        "SCOPES = ['https://www.googleapis.com/auth/drive.readonly']\n",
        "credentials = service_account.Credentials.from_service_account_info(\n",
        "    st.secrets[\"GDRIVE_CREDENTIALS\"],  # loaded from secrets.toml\n",
        "    scopes=SCOPES\n",
        ")\n",
        "\n",
        "# Build service\n",
        "service = build('drive', 'v3', credentials=credentials)\n",
        "\n",
        "# File ID\n",
        "file_id = '1JJBW3fE8GZLVZQgPt1CS7HSYWeB2uqE8-p9xtIReIxU'\n",
        "\n",
        "# Download file\n",
        "request = service.files().get_media(fileId=file_id)\n",
        "fh = io.BytesIO()\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while not done:\n",
        "    _, done = downloader.next_chunk()\n",
        "\n",
        "fh.seek(0)\n",
        "df = pd.read_csv(fh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woymu7-JpIq7",
        "outputId": "fddcbf7c-9a1b-488a-9dc1-74e13f2939ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "# config\n",
        "BIOBERT_MODEL = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "EMBED_DIM = 768  # BioBERT output dim\n",
        "# load models\n",
        "tokenizer_biobert = AutoTokenizer.from_pretrained(BIOBERT_MODEL)\n",
        "biobert = AutoModel.from_pretrained(BIOBERT_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljKV5aJcpNGT"
      },
      "source": [
        "# 3. ClinicalBERT Embedding Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCiaVFg3pMjG",
        "outputId": "218adb7b-b0de-42c2-ad45-64ec138941ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "def get_embedding(text: str) -> np.ndarray:\n",
        "    inputs = tokenizer_biobert(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = biobert(**inputs)\n",
        "    attention_mask = inputs['attention_mask']\n",
        "    last_hidden = outputs.last_hidden_state\n",
        "    mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
        "    masked_output = last_hidden * mask\n",
        "    mean_pooled = masked_output.sum(1) / mask.sum(1)\n",
        "    return mean_pooled.squeeze().cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyFm1W-hpdIy"
      },
      "source": [
        "# 4. FAISS Index Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrvn2QsRpe_d",
        "outputId": "7b4f86e0-22a0-4585-ecca-84ce56a383e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "\n",
        "def build_faiss_index(embeddings: List[np.ndarray]) -> faiss.IndexFlatL2:  # Change to faiss.IndexFlat\n",
        "    \"\"\"\n",
        "    Builds a FAISS index using the provided embeddings.\n",
        "\n",
        "    Args:\n",
        "        embeddings: A list of NumPy arrays representing the embeddings.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS index object (faiss.IndexFlat).\n",
        "    \"\"\"\n",
        "    dimension = embeddings[0].shape[0]  # Get dimensionality from embeddings\n",
        "    index = faiss.IndexFlat(dimension)  # Change to faiss.IndexFlat\n",
        "    index.add(np.vstack(embeddings))  # Add embeddings to the index\n",
        "    return index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGEQcbk_pg0e"
      },
      "source": [
        "## 5. Text Chunking\n",
        "Using Note Sections/Headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuULSKH1piyD",
        "outputId": "9b7fba9a-9710-4cb1-baef-1bd24c8036e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "def chunk_text(input_text: str, charttime, max_chars: int = 500) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Chunks a clinical text into smaller pieces based on sections and a maximum\n",
        "    character limit.\n",
        "\n",
        "    Args:\n",
        "        input_text: The clinical text to be chunked.\n",
        "        max_chars: The maximum number of characters allowed in each sub-chunk.\n",
        "\n",
        "    Returns:\n",
        "        A list of (section, chunk text, date) tuples.\n",
        "    \"\"\"\n",
        "\n",
        "    section_headers = [\n",
        "        \"Service:\", \"Allergies:\",\n",
        "        \"Chief Complaint:\", \"Major Surgical or Invasive Procedure:\", \"History of Present Illness:\",\n",
        "        \"Past Medical History:\", \"Social History:\", \"Family History:\", \"Physical Exam:\",\n",
        "        \"PHYSICAL EXAM ON ADMISSION:\", \"PHYSICAL EXAM ON DISCHARGE:\", \"Pertinent Results:\",\n",
        "        \"Brief Hospital Course:\", \"Medications on Admission:\", \"Discharge Medications:\",\n",
        "        \"Discharge Disposition:\", \"Facility:\", \"Discharge Diagnosis:\", \"Discharge Condition:\",\n",
        "        \"Discharge Instructions:\", \"Followup Instructions:\"\n",
        "    ]\n",
        "\n",
        "    pattern = re.compile(rf\"^({'|'.join(map(re.escape, section_headers))})\", re.MULTILINE)\n",
        "    matches = list(pattern.finditer(input_text))\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(len(matches)):\n",
        "        start = matches[i].start()\n",
        "        end = matches[i + 1].start() if i + 1 < len(matches) else len(input_text)\n",
        "        header = matches[i].group(1)\n",
        "        content = input_text[start:end].strip()\n",
        "\n",
        "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', content)\n",
        "        current_chunk = \"\"\n",
        "        for sentence in sentences:\n",
        "            if len(current_chunk) + len(sentence) <= max_chars:\n",
        "                current_chunk += sentence\n",
        "            else:\n",
        "                chunks.append((header, current_chunk.strip(), charttime))\n",
        "                current_chunk = sentence\n",
        "    chunks.append((header, current_chunk.strip(), charttime))\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2IGK_sPpsd0"
      },
      "source": [
        "chunk structure:\n",
        "```\n",
        "all_chunks = {\n",
        "  # patient  : list of their note chunks + metadata\n",
        "  subject_id : [(section, chunk text, time)]\n",
        "  \n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1mHdUPtpv2Z"
      },
      "source": [
        "## 7. Querying with FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YesE_rxDpzdd",
        "outputId": "0ff44481-4cf5-455f-b1b7-14d0aa443e41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "# Constants\n",
        "SCORE_THRESHOLD = 50   # Adjust based on empirical observation\n",
        "MAX_CHUNKS = 15\n",
        "\n",
        "def search_chunks_with_faiss(all_chunks, subject_id, query, top_k=30):\n",
        "    \"\"\"\n",
        "    Searches FAISS and filters chunks by score and count.\n",
        "    Assumes all_chunks[subject_id] = list of tuples (text, date).\n",
        "    \"\"\"\n",
        "    all_texts_with_dates = all_chunks.get(subject_id, [])\n",
        "    if not all_texts_with_dates:\n",
        "        print(f\"No chunks found for subject ID: {subject_id}\")\n",
        "        return []\n",
        "\n",
        "    # Build index using only the chunk text part of the tuples\n",
        "    all_texts = [text for section, text, date in all_texts_with_dates]\n",
        "\n",
        "    # embed all chunks\n",
        "    embeddings = [get_embedding(text) for text in all_texts]\n",
        "    index = build_faiss_index(embeddings) # build FAISS index from embeddings\n",
        "\n",
        "    # Search\n",
        "    query_emb = get_embedding(query) #embed query and use to search\n",
        "    D, I = index.search(query_emb.reshape(1, -1), top_k)\n",
        "\n",
        "    # Filter by score and cap to max chunks, include date\n",
        "    results = []\n",
        "    for i, score in zip(I[0], D[0]):\n",
        "        if score <= SCORE_THRESHOLD:\n",
        "            # Get section, text, and date from the original list using i\n",
        "            section, text, date = all_texts_with_dates[i]\n",
        "            results.append((text, section, date, score))\n",
        "            if len(results) >= MAX_CHUNKS:\n",
        "                break\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guRhgP5op4dG"
      },
      "source": [
        "## 8. Response Generation with LLaMA\n",
        " + display in streamlit app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxTJEW9up4Lh",
        "outputId": "6600bbd0-7388-4feb-f9c9-0805ff3a18ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "os.environ[\"TOGETHER_API_KEY\"] = os.environ[\"TOGETHER_API_KEY\"]\n",
        "client = Together(api_key=os.environ[\"TOGETHER_API_KEY\"])\n",
        "MODEL_NAME = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "By3Dxh8sqNvX",
        "outputId": "25a7a4d4-753b-419a-ef1f-1edc3ee35743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "def generate_response_from_chunks(query: str, retrieved_chunks: List[tuple]) -> str:\n",
        "    \"\"\"\n",
        "    Generates a clinically relevant, faithful, and date-aware summary using retrieved context.\n",
        "    \"\"\"\n",
        "    # Format each chunk as a date-tagged clinical note\n",
        "    context = \"\\n\".join([f\"[{section} {date}] {text.strip()}\" for text, section, date, score in retrieved_chunks])\n",
        "\n",
        "    # Optimized prompt\n",
        "    prompt = f\"\"\"You are a clinical assistant helping summarize a patient's medical history for a physician during clinical assessment.\n",
        "\n",
        "Patient Timeline (each entry includes a section, date, and note):\n",
        "{context}\n",
        "\n",
        "Query: \"{query}\"\n",
        "\n",
        "Instructions:\n",
        "- Extract only relevant and factual information from the timeline.\n",
        "- Summarize findings related to the query in clinical terms.\n",
        "- Associate each finding with its date.\n",
        "- Do not hallucinate or infer conditions that are not explicitly mentioned.\n",
        "- Present the response in a clear, concise manner suitable for use in clinical decision-making.\n",
        "\n",
        "Answer:\"\"\"\n",
        "    # Print the length of the prompt\n",
        "    print(\"== PROMPT:==\")\n",
        "    print(prompt)\n",
        "\n",
        "    # Print the length of the prompt\n",
        "    print(f\"Prompt length: {len(prompt)}\")\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsGSce7oqBhs",
        "outputId": "437c893c-bc73-41f8-e5b7-a1df44763a84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "\n",
        "# Streamlit app structure\n",
        "st.set_page_config(page_title=\"Clinical Chatbot\", layout=\"centered\")\n",
        "st.title(\"🩺 Clinical Chatbot Assistant\")\n",
        "\n",
        "# Subject ID input (fixed or from dropdown in future)\n",
        "subject_id_to_search = st.number_input(\"Patient Subject ID\", value=10001217)\n",
        "\n",
        "# Initialize session state\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Initialize chunk dictionary only once\n",
        "if \"all_chunks\" not in st.session_state:\n",
        "    all_chunks = {}\n",
        "    for index, row in df.iterrows():\n",
        "        subject_id = row['subject_id']\n",
        "        if subject_id == subject_id_to_search:\n",
        "            note = row['text']\n",
        "            charttime = row['charttime']\n",
        "            chunks = chunk_text(note, charttime)\n",
        "            all_chunks[subject_id] = chunks\n",
        "    st.session_state.all_chunks = all_chunks\n",
        "\n",
        "# Display chat history\n",
        "for msg in st.session_state.messages:\n",
        "    with st.chat_message(msg[\"role\"]):\n",
        "        st.markdown(msg[\"content\"])\n",
        "\n",
        "# Chat input box\n",
        "user_query = st.chat_input(\"Enter your clinical question...\")\n",
        "\n",
        "if user_query:\n",
        "    # Display user message\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_query})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(user_query)\n",
        "\n",
        "    # Process user query\n",
        "    with st.spinner(\"Searching notes and generating response...\"):\n",
        "        retrieved_chunks = search_chunks_with_faiss(\n",
        "            st.session_state.all_chunks,\n",
        "            subject_id_to_search,\n",
        "            user_query\n",
        "        )\n",
        "        response = generate_response_from_chunks(user_query, retrieved_chunks)\n",
        "\n",
        "    # Display assistant response\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo7llzaNsCaQ",
        "outputId": "3b54d680-b06a-43bd-db9e-80b547a23600"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 2s\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://calm-oranges-send.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run app.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JDv9IcLxvvr"
      },
      "source": [
        "Testing Inputs:\n",
        "- Password: 34.46.47.233\n",
        "\n",
        "- Query: Does the patient show signs of neurological issues like face numbness?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
