{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXBQHvhko8_q",
        "outputId": "261c4a44-44b8-434a-9ab7-4d1a804adf75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "## 1. Installation and Setup\n",
        "import streamlit as st\n",
        "import os\n",
        "import faiss\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import io\n",
        "from typing import List, Tuple\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from together import Together\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsD2D8JDo9l9",
        "outputId": "0e81fdad-d4ee-4110-8445-43e1960926c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "# Configs \n",
        "st.set_page_config(page_title=\"Clinical Chatbot\", layout=\"centered\")\n",
        "BIOBERT_MODEL = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "EMBED_DIM = 768\n",
        "MODEL_NAME = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
        "SCORE_THRESHOLD = 50\n",
        "MAX_CHUNKS = 15\n",
        "file_id = '1JJBW3fE8GZLVZQgPt1CS7HSYWeB2uqE8-p9xtIReIxU'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "# Set environment variables\n",
        "os.environ[\"TOGETHER_API_KEY\"] = st.secrets[\"TOGETHER_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "# Load models with cache\n",
        "@st.cache_resource\n",
        "def load_biobert():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BIOBERT_MODEL)\n",
        "    model = AutoModel.from_pretrained(BIOBERT_MODEL)\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer_biobert, biobert = load_biobert()\n",
        "\n",
        "@st.cache_resource\n",
        "def get_bigquery_client():\n",
        "    credentials_info = st.secrets[\"BIGQUERY_CREDENTIALS\"]  # Add to .streamlit/secrets.toml\n",
        "    credentials = service_account.Credentials.from_service_account_info(credentials_info)\n",
        "    return bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
        "\n",
        "@st.cache_data(ttl=3600)\n",
        "def query_discharge_notes(subject_id: int):\n",
        "    client = get_bigquery_client()\n",
        "    query = f\"\"\"\n",
        "        SELECT subject_id, charttime, text\n",
        "        FROM `adelaide-api.clinical_RAG.discharge_notes_40_patients`\n",
        "        WHERE subject_id = {subject_id}\n",
        "        ORDER BY charttime DESC\n",
        "    \"\"\"\n",
        "    df = client.query(query).to_dataframe()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "@st.cache_data\n",
        "def chunk_text(note: str, charttime, max_chars: int = 500) -> List[Tuple[str, str, str]]:\n",
        "    section_headers = [\n",
        "        \"Service:\", \"Allergies:\", \"Chief Complaint:\",\n",
        "        \"Major Surgical or Invasive Procedure:\", \"History of Present Illness:\",\n",
        "        \"Past Medical History:\", \"Social History:\", \"Family History:\",\n",
        "        \"Physical Exam:\", \"PHYSICAL EXAM ON ADMISSION:\", \"PHYSICAL EXAM ON DISCHARGE:\",\n",
        "        \"Pertinent Results:\", \"Brief Hospital Course:\", \"Medications on Admission:\",\n",
        "        \"Discharge Medications:\", \"Discharge Disposition:\", \"Facility:\",\n",
        "        \"Discharge Diagnosis:\", \"Discharge Condition:\", \"Discharge Instructions:\", \"Followup Instructions:\"\n",
        "    ]\n",
        "    pattern = re.compile(rf\"^({'|'.join(map(re.escape, section_headers))})\", re.MULTILINE)\n",
        "    matches = list(pattern.finditer(note))\n",
        "    chunks = []\n",
        "    for i in range(len(matches)):\n",
        "        start = matches[i].start()\n",
        "        end = matches[i + 1].start() if i + 1 < len(matches) else len(note)\n",
        "        header = matches[i].group(1)\n",
        "        content = note[start:end].strip()\n",
        "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', content)\n",
        "        current_chunk = \"\"\n",
        "        for sentence in sentences:\n",
        "            if len(current_chunk) + len(sentence) <= max_chars:\n",
        "                current_chunk += sentence\n",
        "            else:\n",
        "                chunks.append((header, current_chunk.strip(), charttime))\n",
        "                current_chunk = sentence\n",
        "    chunks.append((header, current_chunk.strip(), charttime))\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "@st.cache_data\n",
        "def get_embedding(text: str) -> np.ndarray:\n",
        "    inputs = tokenizer_biobert(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = biobert(**inputs)\n",
        "    attention_mask = inputs['attention_mask']\n",
        "    last_hidden = outputs.last_hidden_state\n",
        "    mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
        "    masked_output = last_hidden * mask\n",
        "    mean_pooled = masked_output.sum(1) / mask.sum(1)\n",
        "    return mean_pooled.squeeze().cpu().numpy()\n",
        "\n",
        "def build_faiss_index(embeddings: List[np.ndarray]) -> faiss.IndexFlat:\n",
        "    dimension = embeddings[0].shape[0]\n",
        "    index = faiss.IndexFlat(dimension)\n",
        "    index.add(np.vstack(embeddings))\n",
        "    return index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "def search_chunks_with_faiss(all_chunks, subject_id, query, top_k=30):\n",
        "    chunks = all_chunks.get(subject_id, [])\n",
        "    if not chunks:\n",
        "        return []\n",
        "    texts = [text for section, text, date in chunks]\n",
        "    embeddings = [get_embedding(t) for t in texts]\n",
        "    index = build_faiss_index(embeddings)\n",
        "    query_emb = get_embedding(query)\n",
        "    D, I = index.search(query_emb.reshape(1, -1), top_k)\n",
        "    results = []\n",
        "    for i, score in zip(I[0], D[0]):\n",
        "        if score <= SCORE_THRESHOLD:\n",
        "            section, text, date = chunks[i]\n",
        "            results.append((text, section, date, score))\n",
        "            if len(results) >= MAX_CHUNKS:\n",
        "                break\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "client = Together(api_key=os.environ[\"TOGETHER_API_KEY\"])\n",
        "\n",
        "def generate_response_from_chunks(query: str, retrieved_chunks: List[tuple]) -> str:\n",
        "    context = \"\\n\".join([f\"[{section} {date}] {text.strip()}\" for text, section, date, score in retrieved_chunks])\n",
        "    prompt = f\"\"\"You are a clinical assistant helping summarize a patient's medical history for a physician during clinical assessment.\n",
        "\n",
        "Patient Timeline:\n",
        "{context}\n",
        "\n",
        "Query: \\\"{query}\\\"\n",
        "\n",
        "Instructions:\n",
        "- Extract only relevant and factual information\n",
        "- Associate each finding with its date\n",
        "- Present in a clear, concise clinical manner\n",
        "\n",
        "Answer:\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# steamlit app layout \n",
        "st.title(\"ðŸ©º Clinical Chatbot Assistant\")\n",
        "subject_id_to_search = st.number_input(\"Patient Subject ID\", value=10001217)\n",
        "df = query_discharge_notes(subject_id_to_search)\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "if \"all_chunks\" not in st.session_state:\n",
        "    all_chunks = {}\n",
        "    for _, row in df.iterrows():\n",
        "        sid = row['subject_id']\n",
        "        if sid == subject_id_to_search:\n",
        "            note, charttime = row['text'], row['charttime']\n",
        "            chunks = chunk_text(note, charttime)\n",
        "            all_chunks[sid] = chunks\n",
        "    st.session_state.all_chunks = all_chunks\n",
        "\n",
        "for msg in st.session_state.messages:\n",
        "    with st.chat_message(msg[\"role\"]):\n",
        "        st.markdown(msg[\"content\"])\n",
        "\n",
        "user_query = st.chat_input(\"Enter your clinical question...\")\n",
        "\n",
        "if user_query:\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_query})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(user_query)\n",
        "    with st.spinner(\"Searching notes and generating response...\"):\n",
        "        chunks = search_chunks_with_faiss(\n",
        "            st.session_state.all_chunks, subject_id_to_search, user_query\n",
        "        )\n",
        "        response = generate_response_from_chunks(user_query, chunks)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "adelaide_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
